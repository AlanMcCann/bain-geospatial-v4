{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "electoral-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import json\n",
    "import geojson\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import csv\n",
    "import copy\n",
    "import ast\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "\n",
    "MAP_VERSION = 3\n",
    "STATS_TYPES = ['Count','Percent','nshft','pshft','growth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b5f1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_codes = {\n",
    " '01': {'abbr': 'AL', 'full_name': 'alabama'},\n",
    " '02': {'abbr': 'AK', 'full_name': 'alaska'},\n",
    " '04': {'abbr': 'AZ', 'full_name': 'arizona'},\n",
    " '05': {'abbr': 'AR', 'full_name': 'arkansas'},\n",
    " '06': {'abbr': 'CA', 'full_name': 'california'},\n",
    " '08': {'abbr': 'CO', 'full_name': 'colorado'},\n",
    " '09': {'abbr': 'CT', 'full_name': 'connecticut'},\n",
    " '10': {'abbr': 'DE', 'full_name': 'delaware'},\n",
    " '11': {'abbr': 'DC', 'full_name': 'district_of_columbia'},\n",
    " '12': {'abbr': 'FL', 'full_name': 'florida'},\n",
    " '13': {'abbr': 'GA', 'full_name': 'georgia'},\n",
    " '15': {'abbr': 'HI', 'full_name': 'hawaii'},\n",
    " '16': {'abbr': 'ID', 'full_name': 'idaho'},\n",
    " '17': {'abbr': 'IL', 'full_name': 'illinois'},\n",
    " '18': {'abbr': 'IN', 'full_name': 'indiana'},\n",
    " '19': {'abbr': 'IA', 'full_name': 'iowa'},\n",
    " '20': {'abbr': 'KS', 'full_name': 'kansas'},\n",
    " '21': {'abbr': 'KY', 'full_name': 'kentucky'},\n",
    " '22': {'abbr': 'LA', 'full_name': 'louisiana'},\n",
    " '23': {'abbr': 'ME', 'full_name': 'maine'},\n",
    " '24': {'abbr': 'MD', 'full_name': 'maryland'},\n",
    " '25': {'abbr': 'MA', 'full_name': 'massachusetts'},\n",
    " '26': {'abbr': 'MI', 'full_name': 'michigan'},\n",
    " '27': {'abbr': 'MN', 'full_name': 'minnesota'},\n",
    " '28': {'abbr': 'MS', 'full_name': 'mississippi'},\n",
    " '29': {'abbr': 'MO', 'full_name': 'missouri'},\n",
    " '30': {'abbr': 'MT', 'full_name': 'montana'},\n",
    " '31': {'abbr': 'NE', 'full_name': 'nebraska'},\n",
    " '32': {'abbr': 'NV', 'full_name': 'nevada'},\n",
    " '33': {'abbr': 'NH', 'full_name': 'new_hampshire'},\n",
    " '34': {'abbr': 'NJ', 'full_name': 'new_jersey'},\n",
    " '35': {'abbr': 'NM', 'full_name': 'new_mexico'},\n",
    " '36': {'abbr': 'NY', 'full_name': 'new_york'},\n",
    " '37': {'abbr': 'NC', 'full_name': 'north_carolina'},\n",
    " '38': {'abbr': 'ND', 'full_name': 'north_dakota'},\n",
    " '39': {'abbr': 'OH', 'full_name': 'ohio'},\n",
    " '40': {'abbr': 'OK', 'full_name': 'oklahoma'},\n",
    " '41': {'abbr': 'OR', 'full_name': 'oregon'},\n",
    " '42': {'abbr': 'PA', 'full_name': 'pennsylvania'},\n",
    " '44': {'abbr': 'RI', 'full_name': 'rhode_island'},\n",
    " '45': {'abbr': 'SC', 'full_name': 'south_carolina'},\n",
    " '46': {'abbr': 'SD', 'full_name': 'south_dakota'},\n",
    " '47': {'abbr': 'TN', 'full_name': 'tennessee'},\n",
    " '48': {'abbr': 'TX', 'full_name': 'texas'},\n",
    " '49': {'abbr': 'UT', 'full_name': 'utah'},\n",
    " '50': {'abbr': 'VT', 'full_name': 'vermont'},\n",
    " '51': {'abbr': 'VA', 'full_name': 'virginia'},\n",
    " '53': {'abbr': 'WA', 'full_name': 'washington'},\n",
    " '54': {'abbr': 'WV', 'full_name': 'west_virginia'},\n",
    " '55': {'abbr': 'WI', 'full_name': 'wisconsin'},\n",
    " '56': {'abbr': 'WY', 'full_name': 'wyoming'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064f837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "773eb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Files\n",
    "# 2 => \"02\"\n",
    "def formatted_state_fips_code(fips_number):\n",
    "    return f'{fips_number:02d}'\n",
    "\n",
    "#  block_group_geojson_filename(state_fips_code=46, year=2010)\n",
    "# tl_2010_46_bg10.json\n",
    "def block_group_geojson_filename(state_fips_code=1, census_year=2010):\n",
    "    year_string = str(census_year)\n",
    "    fips_code_string = formatted_state_fips_code(state_fips_code)\n",
    "    return f'tl_{year_string}_{fips_code_string}_bg10.json'\n",
    "\n",
    "# wa_washington_zip_codes_geo.min.json\n",
    "# def zip_code_geojson_filename(state_fips_code=1):\n",
    "#     formatted_fips_code = formatted_state_fips_code(state_fips_code)\n",
    "#     state_abbr = fips_codes[formatted_fips_code][\"abbr\"].lower()\n",
    "#     state_full_name = fips_codes[formatted_fips_code][\"full_name\"]\n",
    "#     return f'{state_abbr}_{state_full_name}_zip_codes_geo.min.json'\n",
    "\n",
    "\n",
    "# def zip_geojson_files_for_market(market_data):\n",
    "#     fips_codes = json.loads(market_data['StateFips'])\n",
    "#     files = []\n",
    "#     for fips_code in fips_codes:\n",
    "#         files.append(zip_code_geojson_filename(fips_code))\n",
    "#     return files\n",
    "\n",
    "def bg_geojson_files_for_market(market_data, census_year):\n",
    "    fips_codes = json.loads(market_data['StateFips'])\n",
    "    files = []\n",
    "    for fips_code in fips_codes:\n",
    "        files.append(block_group_geojson_filename(state_fips_code=int(fips_code), census_year=census_year))\n",
    "    return files\n",
    "\n",
    "def read_geo_gson_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        gj = geojson.load(f)\n",
    "        features = gj['features']\n",
    "    print(f'Loaded {len(features)} features from {file_path}')\n",
    "    return  features\n",
    "\n",
    "def read_geojson_files(file_paths, base_path = \"./\"):\n",
    "    features = []\n",
    "    for file_path in file_paths:\n",
    "        features += read_geo_gson_file(os.path.join(base_path, file_path))\n",
    "    return features\n",
    "    \n",
    "\n",
    "# def read_market_zip_code_usage(market_file_prefix, base_path=\"./source_data\" ):\n",
    "#     data_type = \"zip\"\n",
    "#     csv_file_path = mapping_data_file_path(market_file_prefix, base_path=base_path, data_type=data_type)\n",
    "#     data_rows = []\n",
    "#     with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             data_rows.append(row)\n",
    "#     zip_codes = []\n",
    "#     for data_row in data_rows:\n",
    "#         zip_codes.append(data_row['Zip'].zfill(5))\n",
    "#     print(f'Loaded {len(zip_codes)} zip_codes from {csv_file_path}')\n",
    "#     return zip_codes\n",
    "        \n",
    "def read_market_block_group_usage(market_file_prefix, base_path=\"./source_data\" ):\n",
    "    data_type = \"bg\"\n",
    "    csv_file_path = mapping_data_file_path(market_file_prefix, base_path=base_path, data_type=data_type)\n",
    "    data_rows = []\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data_rows.append(row)\n",
    "    block_groups = []\n",
    "    for data_row in data_rows:\n",
    "        block_groups.append(data_row['BG'].zfill(12))\n",
    "    print(f'Loaded {len(block_groups)} block groups from {csv_file_path}')\n",
    "    return block_groups\n",
    "        \n",
    "# market_file_prefix = 'DallasFortWorth'\n",
    "# market_prefix= 'dfw'\n",
    "# DallasFortWorth__Mapping_BG_Data.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_BG_Data.csv - the actual zip code data\n",
    "# \n",
    "# DallasFortWorth__Mapping_Zip_Data.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_Zip_Data.csv - the actual block group data\n",
    "# \n",
    "# DallasFortWorth__Mapping_Settings_Transposed File_BG.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_Settings_Transposed_File_BG.csv - segment definitions for Block Groups\n",
    "# DallasFortWorth__Mapping_Settings_Transposed File_Zip.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_Settings_Transposed_File_Zip.csv - segment definitions for Zip Codes\n",
    "\n",
    "def file_type_for_data_type(data_type = \"bg\"):\n",
    "    if data_type == \"zip\": \n",
    "        return \"Zip\"\n",
    "    if data_type == \"bg\":\n",
    "        return \"BG\"\n",
    "\n",
    "# DallasFortWorth__Mapping_BG_Data.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_BG_Data.csv - the actual zip code data\n",
    "# \n",
    "# DallasFortWorth__Mapping_Zip_Data.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_Zip_Data.csv - the actual block group data\n",
    "# \n",
    "def mapping_data_file_path(market_file_prefix, base_path=\"./source_data\", data_type = \"bg\"):\n",
    "    file_type = file_type_for_data_type(data_type = data_type)\n",
    "    return f'{base_path}/{market_file_prefix}__Mapping_{file_type}_Data.csv'\n",
    "\n",
    "# DallasFortWorth__Mapping_Settings_Transposed File_BG.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_Settings_Transposed_File_BG.csv - segment definitions for Block Groups\n",
    "# DallasFortWorth__Mapping_Settings_Transposed File_Zip.csv\n",
    "# ./source_data/{market_file_prefix}__Mapping_Settings_Transposed_File_Zip.csv - segment definitions for Zip Codes\n",
    "def map_settings_data_file_path(market_file_prefix, base_path=\"./source_data\", data_type = \"bg\"):\n",
    "    file_type = file_type_for_data_type(data_type = data_type)\n",
    "    return f'{base_path}/{market_file_prefix}__Mapping_Settings_Transposed File_{file_type}.csv'\n",
    "\n",
    "def published_map_data_path(market_prefix,  data_type = \"bg\", published_data_base_path=\"./public\"):\n",
    "    return f'{published_data_base_path}/{market_prefix}/data/{market_prefix}_{data_type}_data.json'\n",
    "\n",
    "def published_map_settings_path(market_prefix,  data_type = \"bg\", published_data_base_path=\"./public\"):\n",
    "    return f'{published_data_base_path}/{market_prefix}_{data_type}_settings_data.json'\n",
    "\n",
    "def published_map_labels_path(market_prefix,  data_type = \"bg\", published_data_base_path=\"./public\"):\n",
    "    return f'{published_data_base_path}/{market_prefix}_labels.json'\n",
    "\n",
    "def read_mapping_data(market_file_prefix, base_path='./source_data', data_type='bg'):\n",
    "    data_rows = []\n",
    "    csv_file_path = mapping_data_file_path(market_file_prefix, base_path=base_path, data_type = data_type)\n",
    "    with open(csv_file_path) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data_rows.append(row)\n",
    "    print(f'Loaded {len(data_rows)} data rows from {csv_file_path}')\n",
    "    return data_rows\n",
    "\n",
    "def create_zip_features(features, zip_data_rows):\n",
    "    new_features_to_keep = []\n",
    "    for data_row in zip_data_rows:\n",
    "        zip_code = data_row['Zip'].zfill(5)\n",
    "        # find the associated feature\n",
    "        for feature in features:\n",
    "            if feature['properties'][\"ZCTA5CE10\"] == zip_code:\n",
    "                new_feature = copy.deepcopy(feature)\n",
    "                for key in data_row.keys():\n",
    "                    if key != 'Zip':\n",
    "                        if data_row[key]:\n",
    "                            if data_row[key] != 'inf':\n",
    "                                new_feature['properties'][key] =  ast.literal_eval(data_row[key].replace(',',''))\n",
    "                new_features_to_keep.append(new_feature)\n",
    "                break\n",
    "    print(f'Processed {len(new_features_to_keep)} features to use')\n",
    "    return new_features_to_keep\n",
    "\n",
    "def create_bg_features(features, map_settings_data, bg_data_rows, stats_type):\n",
    "    new_features_to_keep = []\n",
    "    for data_row in bg_data_rows:\n",
    "        block_group = str(data_row['BG']).zfill(12)\n",
    "        # find the associated feature\n",
    "        found = False\n",
    "        for feature in features:\n",
    "            if feature['properties'][\"GEOID10\"] == block_group:\n",
    "                found = True\n",
    "                new_feature = copy.deepcopy(feature)\n",
    "                properties = new_feature['properties']\n",
    "                properties.pop(\"ALAND10\",0)\n",
    "                properties.pop(\"AWATER10\",0)\n",
    "                properties.pop(\"BLKGRPCE10\",0)\n",
    "                properties.pop(\"COUNTYFP10\",0)\n",
    "                properties.pop(\"FUNCSTAT10\",0)\n",
    "                properties.pop(\"INTPTLAT10\",0)\n",
    "                properties.pop(\"INTPTLON10\",0)\n",
    "                properties.pop(\"MTFCC10\",0)\n",
    "                properties.pop(\"NAMELSAD10\",0)\n",
    "                properties.pop(\"STATEFP10\",0)\n",
    "                properties.pop(\"TRACTCE10\",0)\n",
    "                new_feature['properties'] = properties\n",
    "                for key in list(data_row.keys())[0:-1]:\n",
    "                    if key not in  ['BG','LAT','LONG'] and stats_type in key: #and 'Count' in key:\n",
    "                        if data_row[key] and data_row[key] != 'inf':\n",
    "                            translated_key = map_settings_data[key]['mapbox_segment']\n",
    "                            value = ast.literal_eval(data_row[key].replace(',',''))\n",
    "                            value = int(value * 1000000) / 1000000\n",
    "                            new_feature['properties'][translated_key] =  value\n",
    "                        else: \n",
    "                            print('exclude', data_row, key)\n",
    "                new_features_to_keep.append(new_feature)\n",
    "                break\n",
    "    #         if found == False:\n",
    "    #             print(f'failed: {block_group}')\n",
    "    print(f'Processed {len(new_features_to_keep)} features to use')\n",
    "    return new_features_to_keep\n",
    "\n",
    "def create_geojson(features):\n",
    "    print(f'Created GeoJson with  {len(features)} features')\n",
    "    return FeatureCollection(features)\n",
    "\n",
    "def write_geojson(geojson_data, market_prefix, data_type='bg', stats_type='Count',published_data_base_path = \"./public\"):\n",
    "    geojson_string = geojson.dumps(geojson_data, sort_keys=True)\n",
    "    f = open(f'{published_data_base_path}/{market_prefix}_{stats_type.lower()}.json', \"w\")\n",
    "    f.write(geojson_string)\n",
    "    f.close()\n",
    "    print(f'Wrote geojson data to  {published_data_base_path}/{market_prefix}/data/{market_prefix}_{stats_type.lower()}.json')\n",
    "    \n",
    "def read_map_settings(market_file_prefix, base_path=\"./source_data\", data_type = \"bg\"):\n",
    "    data_rows = []\n",
    "    csv_file_path = map_settings_data_file_path(market_file_prefix, base_path=base_path, data_type = data_type)\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data_rows.append(row)\n",
    "    print(f'Loaded {len(data_rows)} data rows from {csv_file_path}')   \n",
    "    return data_rows\n",
    "\n",
    "def write_settings_data(settings_data, market_file_prefix, base_path=\"./public\", data_type = \"bg\"):\n",
    "    processed_settings_data = OrderedDict()\n",
    "    segment_number = 0\n",
    "    for row in settings_data:\n",
    "        segment_number += 1\n",
    "        if row[\"Display_Data_Type\"] == \"HH_Count\":\n",
    "            row[\"Display_Data_Type\"] = \"HH Count\"\n",
    "        mapbox_segment_key = f's{segment_number}'\n",
    "        row['mapbox_segment'] = mapbox_segment_key\n",
    "        processed_settings_data[row['UniqueSeriesString']] = row\n",
    "    json_string = json.dumps(processed_settings_data, indent = 2, sort_keys=False)\n",
    "    settings_data_file_path = published_map_settings_path(market_file_prefix,  data_type = data_type, published_data_base_path=base_path)\n",
    "    f = open(settings_data_file_path, \"w\")\n",
    "    f.write(json_string)\n",
    "    f.close()\n",
    "    print(f'Wrote settings data rows to {settings_data_file_path}') \n",
    "    return processed_settings_data\n",
    "    \n",
    "# def create_zip_labels(market_file_prefix, base_path=\"./source_data\" , data_type = \"bg\" , published_data_base_path=\"./public\"):\n",
    "#     data_rows = []\n",
    "#     #     csv_file_path = './DFW_Mapping_data.csv'\n",
    "#     csv_file_path = mapping_data_file_path(market_file_prefix, base_path=base_path, data_type = \"bg\")\n",
    "#     with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             data_rows.append(row)\n",
    "\n",
    "#     features = []\n",
    "#     for data_row in data_rows:\n",
    "#         feature =  { \n",
    "#             \"type\": \"Feature\", \n",
    "#             \"properties\": {\n",
    "#                 \"id\": data_row['Zip']\n",
    "#             },\n",
    "#             \"geometry\": { \n",
    "#                 \"type\": \"Point\", \n",
    "#                 \"coordinates\": [ float(data_row['intptlong']), float(data_row['intptlat']) ]\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         features.append(feature)\n",
    "#     geo_json = {\n",
    "#         \"type\": \"FeatureCollection\",\n",
    "#         \"features\": features\n",
    "#     }\n",
    "#     json_string = json.dumps(geo_json, indent = 2, sort_keys=True)\n",
    "#     output_file_path = published_map_labels_path(market_file_prefix,  data_type , published_data_base_path=\"./public\")\n",
    "#     f = open(output_file_path, \"w\")\n",
    "#     f.write(json_string)\n",
    "#     f.close()\n",
    "#     print(f'Wrote geojson labels data to  {output_file_path}')\n",
    "\n",
    "def create_bg_labels(market_file_prefix, base_path=\"./source_data\" , data_type = \"bg\" , published_data_base_path=\"./public\"):\n",
    "    data_rows = []\n",
    "    #     csv_file_path = './DFW_Mapping_data.csv'\n",
    "    csv_file_path = mapping_data_file_path(market_file_prefix, base_path=base_path, data_type = \"bg\")\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data_rows.append(row)\n",
    "\n",
    "    features = []\n",
    "    for data_row in data_rows:\n",
    "        feature =  { \n",
    "            \"type\": \"Feature\", \n",
    "            \"properties\": {\n",
    "                \"id\": data_row['BG']\n",
    "            },\n",
    "            \"geometry\": { \n",
    "                \"type\": \"Point\", \n",
    "                \"coordinates\": [ float(data_row['LONG']), float(data_row['LAT']) ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        features.append(feature)\n",
    "    geo_json = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features\n",
    "    }\n",
    "    json_string = json.dumps(geo_json, indent = 2, sort_keys=True)\n",
    "    output_file_path = published_map_labels_path(market_file_prefix,  data_type , published_data_base_path=published_data_base_path)\n",
    "    f = open(output_file_path, \"w\")\n",
    "    f.write(json_string)\n",
    "    f.close()\n",
    "    print(f'Wrote geojson labels data to  {output_file_path}')\n",
    "\n",
    "def create_geojson_settings(csv_file_path, published_data_base_path = \"./public\"):\n",
    "\n",
    "    geospatial_settings = OrderedDict()\n",
    "    csv_file_path = f'{source_data_base_path}/Market_Mapping_Settings.csv'\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            geospatial_settings[row['MarketName']] = row\n",
    "    geojson_string = json.dumps(geospatial_settings, indent=4, sort_keys=True)\n",
    "    f = open(f'{published_data_base_path}/geospatial_settings.json', \"w\")\n",
    "    f.write(geojson_string)\n",
    "    f.close()\n",
    "    print(f'Wrote geospatials settings data to  {published_data_base_path}/geospatial_settings.json')\n",
    "    \n",
    "# def create_data_folder(market_file_prefix, published_data_base_path):\n",
    "#     path = os.path.join(published_data_base_path, market_file_prefix, 'data')\n",
    "#     pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "# def create_labels_folder(market_file_prefix, published_data_base_path):\n",
    "#     path = os.path.join(published_data_base_path, market_file_prefix, 'labels')\n",
    "#     pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# def create_mstds_data_config_file(market_file_prefix, published_data_base_path):\n",
    "#     config_file_contents = {\n",
    "#         \"username\": \"stobieb\",\n",
    "#         \"tilesetSourceId\": f'{market_file_prefix}_bg_data-src',\n",
    "#         \"tilesetSourcePath\": f'{market_file_prefix}_bg_data.jsonl',\n",
    "#         \"tilesetId\": f'{market_file_prefix}_bg_data',\n",
    "#         \"tilesetName\": f'{market_file_prefix}_bg_data'\n",
    "#     }\n",
    "#     config_file_path = os.path.join(published_data_base_path, market_file_prefix,'data', 'mts-config.json' )\n",
    "#     with open(config_file_path, 'w') as config_file:\n",
    "#         config_file.write(json.dumps(config_file_contents, indent=4))\n",
    "    \n",
    "\n",
    "# def create_mstds_labels_config_file(market_file_prefix, published_data_base_path):\n",
    "#     config_file_contents = {\n",
    "#         \"username\": \"stobieb\",\n",
    "#         \"tilesetSourceId\": f'{market_file_prefix}_bg_labels-src',\n",
    "#         \"tilesetSourcePath\": f'{market_file_prefix}_bg_labels.jsonl',\n",
    "#         \"tilesetId\": f'{market_file_prefix}_bg_labels',\n",
    "#         \"tilesetName\": f'{market_file_prefix}_bg_labels'\n",
    "#     }\n",
    "#     config_file_path = os.path.join(published_data_base_path, market_file_prefix,'labels','mts-config.json' )\n",
    "#     with open(config_file_path, 'w') as config_file:\n",
    "#         config_file.write(json.dumps(config_file_contents, indent=4))\n",
    "    \n",
    "\n",
    "    \n",
    "def create_tileset_data_recipe_file(market_file_prefix, published_data_base_path, stats_type):\n",
    "    recipe_file_contents = {\n",
    "      'version': 1,\n",
    "      'layers': {\n",
    "        f'{market_file_prefix}_{stats_type.lower()}': {\n",
    "          'source': f'mapbox://tileset-source/stobieb/{market_file_prefix}_{stats_type.lower()}_src',\n",
    "          'minzoom': 0,\n",
    "          'maxzoom': 13,\n",
    "          'tiles': {\n",
    "            'layer_size': 2500\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    recipe_file_path = os.path.join(published_data_base_path,f'{market_file_prefix}_{stats_type.lower()}_recipe.json' )\n",
    "    with open(recipe_file_path, 'w') as recipe_file:\n",
    "        recipe_file.write(json.dumps(recipe_file_contents, indent=4))\n",
    "    \n",
    "\n",
    "def create_tileset_labels_recipe_file(market_file_prefix, published_data_base_path):\n",
    "    recipe_file_contents = {\n",
    "      'version': 1,\n",
    "      'layers': {\n",
    "        f'{market_file_prefix}_labels': {\n",
    "          'source': f'mapbox://tileset-source/stobieb/{market_file_prefix}_labels_src',\n",
    "          'minzoom': 0,\n",
    "          'maxzoom': 13,\n",
    "            \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    recipe_file_path = os.path.join(published_data_base_path,f'{market_file_prefix}_labels_recipe.json' )\n",
    "    with open(recipe_file_path, 'w') as recipe_file:\n",
    "        recipe_file.write(json.dumps(recipe_file_contents, indent=4))\n",
    "        \n",
    "# def mstds_convert_file(market_file_prefix, published_data_base_path, filetype='data'):\n",
    "#     recipe_file_path = os.path.join(published_data_base_path, market_file_prefix,'labels','mts-recipe.json' )\n",
    "#     market_file_prefix, published_data_base_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "endangered-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_main_geo_data(source_data_base_path, geojson_files_base_path, published_data_base_path, census_year):\n",
    "    csv_file_path = f'{source_data_base_path}/Market_Mapping_Settings.csv'\n",
    "    market_rows = []\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            market_rows.append(row)\n",
    "    market_count = 0\n",
    "    completed_markets = []\n",
    "    for market_data in market_rows: #[0:10]:\n",
    "        # if market_data['MarketName'] in ['Texas','DallasFortWorth']:\n",
    "        if True:\n",
    "            print(\"\")\n",
    "            print(f'Market Data: {market_count + 1}')\n",
    "            market_file_prefix = market_data['MarketName'].lower()\n",
    "            market_prefix = market_file_prefix\n",
    "            #create_data_folder(market_file_prefix,published_data_base_path )\n",
    "            #create_labels_folder(market_file_prefix,published_data_base_path )\n",
    "            #create_mstds_data_config_file(market_file_prefix, published_data_base_path)\n",
    "            #create_mstds_labels_config_file(market_file_prefix, published_data_base_path)\n",
    "            for stats_type in STATS_TYPES:\n",
    "                create_tileset_data_recipe_file(market_file_prefix, published_data_base_path,stats_type)\n",
    "                create_tileset_labels_recipe_file(market_file_prefix, published_data_base_path)\n",
    "            # Zip Processing     \n",
    "            # data_type = 'zip'\n",
    "            # print(f'market: {market_prefix}')\n",
    "            # geojson_files = zip_geojson_files_for_market(market_data)\n",
    "            # zip_codes = read_market_zip_code_usage(market_file_prefix, base_path=source_data_base_path )\n",
    "            # zip_data_rows = read_mapping_data(market_file_prefix, source_data_base_path, data_type)\n",
    "            # features = read_geojson_files(geojson_files, base_path=geojson_files_base_path)\n",
    "            # features_to_use = create_zip_features(features, zip_data_rows)\n",
    "            # new_geojson = create_geojson(features_to_use)\n",
    "            # write_geojson(new_geojson, market_prefix, data_type, published_data_base_path=published_data_base_path)\n",
    "            # map_settings_data = read_map_settings(market_file_prefix, base_path=source_data_base_path, data_type = \"zip\")\n",
    "            # write_settings_data(map_settings_data, market_prefix, base_path=published_data_base_path, data_type = \"zip\")\n",
    "            # create_zip_labels(market_file_prefix, base_path=source_data_base_path , data_type = \"zip\" , published_data_base_path=published_data_base_path)\n",
    "            # Block Group Processing\n",
    "            data_type = 'bg'\n",
    "            map_settings_data = read_map_settings(market_file_prefix, base_path=source_data_base_path, data_type = \"bg\")\n",
    "            map_settings = write_settings_data(map_settings_data, market_prefix, base_path=published_data_base_path, data_type = \"bg\")\n",
    "            # print(map_settings)\n",
    "            geojson_files = bg_geojson_files_for_market(market_data,census_year )\n",
    "            block_groups = read_market_block_group_usage(market_file_prefix, base_path=source_data_base_path )\n",
    "            block_group_rows = read_mapping_data(market_file_prefix, source_data_base_path, data_type)\n",
    "            features = read_geojson_files(geojson_files, base_path=geojson_files_base_path)\n",
    "            for stats_type in STATS_TYPES:\n",
    "                features_to_use = create_bg_features(features, map_settings, block_group_rows, stats_type)\n",
    "                new_geojson = create_geojson(features_to_use)\n",
    "                write_geojson(new_geojson, market_prefix, data_type, stats_type,published_data_base_path=published_data_base_path)\n",
    "            create_bg_labels(market_file_prefix, base_path=source_data_base_path , data_type = \"bg\" , published_data_base_path=published_data_base_path)\n",
    "            market_count += 1\n",
    "    # Finished states - create overall settings file    \n",
    "    create_geojson_settings(csv_file_path, published_data_base_path = \"./public\")\n",
    "    print(f'*** COMPLETED {market_count} markets')\n",
    "    print(completed_markets)\n",
    "                          \n",
    "        \n",
    "# source_data_base_path = \"./source_data\"\n",
    "# source_data_base_path = \"/Users/alanmccann/Dropbox/bain/bain-uploads/v6_7th_run Brian Stobie/\"\n",
    "# published_data_base_path = \"./public\"\n",
    "# geojson_files_base_path = '/Users/alanmccann/Dropbox/bain/map_source_data'\n",
    "# census_year = 2010\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fc83f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chart_json(published_data_base_path = \"./public\"):\n",
    "    csv_file_path = f'{source_data_base_path}/Chart_Data_File.csv'\n",
    "    chart_data_rows = []\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            chart_data_rows.append(row)\n",
    "    data_json = {}\n",
    "    for row in chart_data_rows:\n",
    "        region = row['Region']\n",
    "        # segment = row['Segment']\n",
    "        year = row['Year']\n",
    "        urban = row['Urban']\n",
    "        suburban = row['SubUrban']\n",
    "        exurban = row['ExUrban']\n",
    "        print(region,year)\n",
    "        print(urban, suburban, exurban)\n",
    "        urban = '%.10f' % float(row['Urban'])\n",
    "        suburban = '%.10f' % float(row['SubUrban'])\n",
    "        exurban = '%.10f' % float(row['ExUrban'])\n",
    "        display_region = row['DisplayRegion']\n",
    "        if region not in data_json.keys():\n",
    "            data_json[region] = {'displayRegion': display_region, 'data': {}}\n",
    "        if year not in data_json[region]['data'].keys():\n",
    "            data_json[region]['data'][year] = {}\n",
    "        if 'Urban' not in data_json[region]['data'][year].keys():\n",
    "            data_json[region]['data'][year]['Urban'] = {}\n",
    "            data_json[region]['data'][year]['SubUrban'] = {}\n",
    "            data_json[region]['data'][year]['ExUrban'] = {}\n",
    "        data_json[region]['data'][year]['Urban'] = urban\n",
    "        data_json[region]['data'][year]['SubUrban'] = suburban\n",
    "        data_json[region]['data'][year]['ExUrban'] = exurban\n",
    "    chart_data_string = json.dumps(data_json, indent=4, sort_keys=True)\n",
    "    f = open(f'{published_data_base_path}/market_summary_charts.json', \"w\")\n",
    "    f.write(chart_data_string)\n",
    "    f.close()\n",
    "    print(f'Wrote market summary charts data to  {published_data_base_path}/market_summary_charts.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a4b97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_chart_json(published_data_base_path = \"./public\"):\n",
    "    csv_file_path = f'{source_data_base_path}/Chart_Data_File.csv'\n",
    "    chart_data_rows = []\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            chart_data_rows.append(row)\n",
    "    data_json = {}\n",
    "    for row in chart_data_rows:\n",
    "        region = row['Region']\n",
    "        display_region = row['DisplayRegion']\n",
    "        year = row['Year']\n",
    "        age_segment=row['Display_Age_Segment']\n",
    "        income_segment=row['Display_Income_Segment']\n",
    "        key = f'{region}-{year}-{age_segment}-{income_segment}'\n",
    "        urban = row['Urban']\n",
    "        suburban = row['SubUrban']\n",
    "        exurban = row['ExUrban']\n",
    "        print(key)\n",
    "        urban = '%.10f' % float(row['Urban'])\n",
    "        suburban = '%.10f' % float(row['SubUrban'])\n",
    "        exurban = '%.10f' % float(row['ExUrban'])\n",
    "        display_region = row['DisplayRegion']\n",
    "        isMetro = row['IsMetro']\n",
    "        needsCaveat = row['NeedsCaveat']\n",
    "        data_json[key] = {\n",
    "            \"year\": year,\n",
    "            \"ageSegment\": age_segment,\n",
    "            \"incomeSegment\": income_segment,\n",
    "            \"urban\": urban,\n",
    "            \"suburban\": suburban,\n",
    "            \"exurban\": exurban,\n",
    "            \"isMetro\": isMetro,\n",
    "            \"needsCaveat\": needsCaveat\n",
    "        }\n",
    "    chart_data_string = json.dumps(data_json, indent=4, sort_keys=True)\n",
    "    f = open(f'{published_data_base_path}/market_comparison_charts.json', \"w\")\n",
    "    f.write(chart_data_string)\n",
    "    f.close()\n",
    "    print(f'Wrote market summary charts data to  {published_data_base_path}/market_comparison_charts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c71a3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_options_json(published_data_base_path = \"./public\"):\n",
    "    csv_file_path = f'{source_data_base_path}/Chart_Data_File.csv'\n",
    "    chart_data_rows = []\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            chart_data_rows.append(row)\n",
    "    year_options_set = set()\n",
    "    age_segment_options_set = set()\n",
    "    income_segment_options_set = set()\n",
    "    region_options_list = []\n",
    "    for row in chart_data_rows:\n",
    "        year_options_set.add(row[\"Year\"])\n",
    "        age_segment_options_set.add(row[\"Display_Age_Segment\"])\n",
    "        income_segment_options_set.add(row[\"Display_Income_Segment\"])\n",
    "        region = {\n",
    "            \"region\": row[\"Region\"],\n",
    "            \"displayRegion\": row[\"DisplayRegion\"],\n",
    "            \"isMetro\": row[\"IsMetro\"],\n",
    "            \"needsCaveat\": row[\"NeedsCaveat\"],\n",
    "        }\n",
    "        region_options_list.append(region)\n",
    "    year_options_list = list(year_options_set)\n",
    "    year_options_list.sort()\n",
    "    age_segment_options_list = list(age_segment_options_set)\n",
    "    age_segment_options_list.sort()\n",
    "    income_segment_options_list = list(income_segment_options_set)\n",
    "    income_segment_options_list.sort()\n",
    "    region_options = pd.DataFrame(region_options_list).drop_duplicates().to_dict('r')\n",
    "    dataOptions = {\n",
    "        \"yearOptions\": year_options_list,\n",
    "        'ageSegmentOptions': age_segment_options_list,\n",
    "        'incomeSegmentOptions': income_segment_options_list,\n",
    "        \"regionOptions\": region_options\n",
    "    }\n",
    "    data_options_string = json.dumps(dataOptions, indent=4, sort_keys=True)\n",
    "    f = open(f'{published_data_base_path}/data_options.json', \"w\")\n",
    "    f.write(data_options_string)\n",
    "    f.close()\n",
    "    print(f'Wrote data options data to  {published_data_base_path}/data_options.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee7f396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Market Data: 1\n",
      "Loaded 265 data rows from /Users/alanmccann/Dropbox/bain/bain-uploads/v6_7th_run Brian Stobie//abilenetx__Mapping_Settings_Transposed File_BG.csv\n",
      "Wrote settings data rows to /Users/alanmccann/Dropbox/bain/mtsds-data/abilenetx_bg_settings_data.json\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dh/fn0cs6ln6sq0722wd5lcgdcm0000gn/T/ipykernel_45875/741845500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Process the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprocess_main_geo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeojson_files_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublished_data_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcensus_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# create_chart_json(published_data_base_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# create_comparison_chart_json(published_data_base_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dh/fn0cs6ln6sq0722wd5lcgdcm0000gn/T/ipykernel_45875/1904803634.py\u001b[0m in \u001b[0;36mprocess_main_geo_data\u001b[0;34m(source_data_base_path, geojson_files_base_path, published_data_base_path, census_year)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mmap_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_settings_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_settings_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarket_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpublished_data_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# print(map_settings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mgeojson_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbg_geojson_files_for_market\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcensus_year\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mblock_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_market_block_group_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_file_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_data_base_path\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mblock_group_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_mapping_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_file_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_data_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dh/fn0cs6ln6sq0722wd5lcgdcm0000gn/T/ipykernel_45875/1861018246.py\u001b[0m in \u001b[0;36mbg_geojson_files_for_market\u001b[0;34m(market_data, census_year)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbg_geojson_files_for_market\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcensus_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfips_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StateFips'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfips_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfips_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Set up input and output locations\n",
    "# Geojson file path\n",
    "geojson_files_base_path = '/Users/alanmccann/Dropbox/bain/map_source_data'\n",
    "# Source csv data path\n",
    "source_data_base_path = \"/Users/alanmccann/Dropbox/bain/bain-uploads/v6_7th_run Brian Stobie/\"\n",
    "# Output data path - all of the files in this folder should be uploaded to the cms GeoSpatialTool/v1 folder\n",
    "published_data_base_path = \"./public\"\n",
    "published_data_base_path = '/Users/alanmccann/Dropbox/bain/mtsds-data'\n",
    "# Census year for geojson\n",
    "census_year = 2010\n",
    "\n",
    "# Process the data\n",
    "process_main_geo_data(source_data_base_path, geojson_files_base_path, published_data_base_path, census_year)\n",
    "# create_chart_json(published_data_base_path)\n",
    "# create_comparison_chart_json(published_data_base_path)\n",
    "create_data_options_json(published_data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c970796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "source_data_base_path = \"/Users/alanmccann/Dropbox/bain/bain-uploads/v6_7th_run Brian Stobie/\"\n",
    "published_data_base_path = '/Users/alanmccann/Dropbox/bain/mtsds-data'\n",
    "csv_file_path = f'{source_data_base_path}/Market_Mapping_Settings.csv'\n",
    "market_rows = []\n",
    "with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        market_rows.append(row)\n",
    "market_count = 0\n",
    "completed_markets = []\n",
    "for market_data in market_rows[2:-1]:\n",
    "    market_file_prefix = market_data['MarketName'].lower()\n",
    "    # if market_data['MarketName'] in ['Texas','DallasFortWorth']:\n",
    "    if True:\n",
    "        for stats_type in STATS_TYPES:\n",
    "            #print(f'tilesets upload-source stobieb {market_file_prefix}_bg_{stats_type.lower()}_data_src {market_file_prefix}_bg_{stats_type.lower()}_data.json --replace')print(f'tilesets upload-source stobieb {market_file_prefix}_bg_{stats_type.lower()}_data_src {market_file_prefix}_bg_{stats_type.lower()}_data.json --replace')\n",
    "            upload_source_command = [\n",
    "                'tilesets',\n",
    "                'upload-source',\n",
    "                'stobieb',\n",
    "                f'{market_file_prefix}_bg_{stats_type.lower()}_data_src',\n",
    "                f'{market_file_prefix}_bg_{stats_type.lower()}_data.json',\n",
    "                '--replace'\n",
    "            ]\n",
    "            # print(f'tilesets create stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data --recipe {market_file_prefix}_bg_{stats_type.lower()}_recipe.json --name \"{market_file_prefix}_bg_{stats_type.lower()}_data\"')\n",
    "            create_tileset_command = [\n",
    "                'tilesets',\n",
    "                'create',\n",
    "                f'stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data',\n",
    "                '--recipe', f'{market_file_prefix}_bg_{stats_type.lower()}_recipe.json'            ]\n",
    "            \n",
    "            \n",
    "            # print(f'tilesets publish stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data')\n",
    "            publish_tileset_command = [\n",
    "                'tilesets',\n",
    "                'publish',\n",
    "                f'stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data'\n",
    "            ]\n",
    "            \n",
    "        print(f'tilesets upload-source stobieb {market_file_prefix}_bg_labels_src {market_file_prefix}_bg_labels.json --replace')\n",
    "        print(f'tilesets create stobieb.{market_file_prefix}_bg_labels --recipe {market_file_prefix}_labels_recipe.json --name \"{market_file_prefix}_bg_labels\"')\n",
    "        # print(f'tilesets publish stobieb.{market_file_prefix}_bg_labels')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac80522",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  {\"ALAND10\": 267278, \"AWATER10\": 0, \"BLKGRPCE10\": \"1\", \n",
    "      \n",
    "      \n",
    "      \"COUNTYFP10\": \"003\", \"FUNCSTAT10\": \"S\", \"GEOID10\": \"250039001001\", \"INTPTLAT10\n",
    "      \n",
    "      \": \"+42.4545103\", \"INTPTLON10\": \"-073.2493561\", \"MTFCC10\": \"G5030\", \"NAMELSAD10\": \"Block Group 1\", \"STATEFP10\": \"25\", \"TRACTCE10\": \"900100\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.pop(\"ALAND10\",0)\n",
    "a.pop(\"AWATER10\",0)\n",
    "a.pop(\"BLKGRPCE10\",0)\n",
    "a.pop(\"COUNTYFP10\",0)\n",
    "a.pop(\"FUNCSTAT10\",0)\n",
    "a.pop(\"INTPTLAT10\",0)\n",
    "a.pop(\"INTPTLON10\",0)\n",
    "a.pop(\"MTFCC10\",0)\n",
    "a.pop(\"NAMELSAD10\",0)\n",
    "a.pop(\"STATEFP10\",0)\n",
    "a.pop(\"TRACTCE10\",0)\n",
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b10e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = {\"geometry\": {\"coordinates\": [[[-73.252424, 42.454116], [-73.252573, 42.453586], [-73.252649, 42.453313], [-73.252756, 42.452939], [-73.25289, 42.45243], [-73.252983, 42.452068], [-73.253156, 42.451533], [-73.252985, 42.451566], [-73.251386, 42.451872], [-73.250079, 42.452057], [-73.249772, 42.452108], [-73.248181, 42.452339], [-73.247938, 42.452368], [-73.246949, 42.452516], [-73.246869, 42.45265], [-73.246869, 42.452745], [-73.24681, 42.452782], [-73.246745, 42.452855], [-73.246709, 42.452962], [-73.246534, 42.453596], [-73.246172, 42.454897], [-73.246012, 42.45537], [-73.245866, 42.455916], [-73.245646, 42.456703], [-73.246754, 42.456868], [-73.248612, 42.457159], [-73.250365, 42.457433], [-73.25057, 42.457181], [-73.250935, 42.456749], [-73.25107, 42.45659], [-73.251254, 42.456346], [-73.251579, 42.455873], [-73.251909, 42.455281], [-73.252182, 42.45478], [-73.252409, 42.45417], [-73.252424, 42.454116]]], \"type\": \"Polygon\"}, \"properties\": {\"ALAND10\": 267278, \"AWATER10\": 0, \"BLKGRPCE10\": \"1\", \"COUNTYFP10\": \"003\", \"FUNCSTAT10\": \"S\", \"GEOID10\": \"250039001001\", \"INTPTLAT10\": \"+42.4545103\", \"INTPTLON10\": \"-073.2493561\", \"MTFCC10\": \"G5030\", \"NAMELSAD10\": \"Block Group 1\", \"STATEFP10\": \"25\", \"TRACTCE10\": \"900100\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a47b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature['properties'].pop(\"ALAND10\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f349b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature[\"properties\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3212088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_mapbox(source_data_base_path,published_data_base_path):\n",
    "    csv_file_path = f'{source_data_base_path}/Market_Mapping_Settings.csv'\n",
    "    market_rows = []\n",
    "    with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            market_rows.append(row)\n",
    "    market_count = 0\n",
    "    completed_markets = []\n",
    "    for market_data in market_rows: #[0:10]:\n",
    "        # if market_data['MarketName'] in ['Texas','DallasFortWorth']:\n",
    "        if True:\n",
    "            print(\"\")\n",
    "            print(f'Market Data: {market_count + 1}')\n",
    "            market_file_prefix = market_data['MarketName'].lower()\n",
    "            os.system(f'cd {published_data_base_path}; tilesets upload-source stobieb {market_file_prefix}_bg_data-src {market_file_prefix}_bg_data.json')\n",
    "            os.system(f'cd {published_data_base_path};  tilesets create stobieb.{market_file_prefix}_bg_data --recipe {market_file_prefix}-bg-recipe.json --name \"{market_file_prefix}_bg_data\"')\n",
    "            os.system(f'cd {published_data_base_path};  tilesets publish stobieb.{market_file_prefix}_bg_data')\n",
    "            os.system(f'cd {published_data_base_path};  tilesets upload-source stobieb {market_file_prefix}_bg_labels-src {market_file_prefix}_bg_labels.json')\n",
    "            os.system(f'cd {published_data_base_path};  tilesets create stobieb.{market_file_prefix}_bg_labels --recipe {market_file_prefix}-labels-recipe.json --name \"{market_file_prefix}_bg_labels\"')\n",
    "            os.system(f'cd {published_data_base_path};  tilesets publish stobieb.{market_file_prefix}_bg_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_base_path = \"/Users/alanmccann/Dropbox/bain/bain-uploads/v6_7th_run Brian Stobie/\"\n",
    "published_data_base_path = '/Users/alanmccann/Dropbox/bain/mtsds-data'\n",
    "upload_to_mapbox(source_data_base_path,published_data_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72eda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "MAPBOX_ACCESS_TOKEN='sk.eyJ1Ijoic3RvYmllYiIsImEiOiJjbDR2dDlibHkwODhjM2lub3EwOXJld2dzIn0.Ur59yGDIvj60ELA7QOCnqQ'\n",
    "my_env = {**os.environ, 'MAPBOX_ACCESS_TOKEN':'sk.eyJ1Ijoic3RvYmllYiIsImEiOiJjbDR2dDlibHkwODhjM2lub3EwOXJld2dzIn0.Ur59yGDIvj60ELA7QOCnqQ'}\n",
    "\n",
    "import subprocess\n",
    "# tilesets job stobieb.dallasfortworth_bg_data cl9kgdofo000209l52zyy3mdi\n",
    "process = subprocess.Popen(['tilesets', 'job', 'stobieb.dallasfortworth_bg_data', 'cl9kgdofo000209l52zyy3mdi'],\n",
    "                     stdout=subprocess.PIPE, \n",
    "                     stderr=subprocess.PIPE,\n",
    "                     env=my_env)\n",
    "stdout, stderr = process.communicate()\n",
    "output = stdout.decode('utf-8').split('\\n')\n",
    "error = stderr.decode('utf-8')\n",
    "result = json.loads(output[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "MAPBOX_ACCESS_TOKEN='sk.eyJ1Ijoic3RvYmllYiIsImEiOiJjbDR2dDlibHkwODhjM2lub3EwOXJld2dzIn0.Ur59yGDIvj60ELA7QOCnqQ'\n",
    "my_env = {**os.environ, 'MAPBOX_ACCESS_TOKEN':'sk.eyJ1Ijoic3RvYmllYiIsImEiOiJjbDR2dDlibHkwODhjM2lub3EwOXJld2dzIn0.Ur59yGDIvj60ELA7QOCnqQ'}\n",
    "published_data_base_path = '/Users/alanmccann/Dropbox/bain/mtsds-data'\n",
    "import subprocess\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.chdir(published_data_base_path)\n",
    "\n",
    "market_file_prefix = 'austin'\n",
    "stats_type = 'Count'\n",
    "\n",
    "def upload_source(market_file_prefix, stats_type, working_directory):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(working_directory)\n",
    "    upload_source_command = [\n",
    "        'tilesets',\n",
    "        'upload-source',\n",
    "        'stobieb',\n",
    "        f'{market_file_prefix}_{stats_type.lower()}_src',\n",
    "        f'{market_file_prefix}_{stats_type.lower()}.json',\n",
    "        '--replace'\n",
    "    ]\n",
    "    print(' '.join(upload_source_command))\n",
    "    process = subprocess.Popen(upload_source_command,\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE,\n",
    "                         env=my_env)\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    output = stdout.decode('utf-8').split('\\n')\n",
    "    error = stderr.decode('utf-8')\n",
    "    print(output)\n",
    "\n",
    "def upload_label_source(market_file_prefix, working_directory):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(working_directory)\n",
    "    upload_source_command = [\n",
    "        'tilesets',\n",
    "        'upload-source',\n",
    "        'stobieb',\n",
    "        f'{market_file_prefix}_labels_src',\n",
    "        f'{market_file_prefix}_labels.json',\n",
    "        #'--replace'\n",
    "    ]\n",
    "    print(' '.join(upload_source_command))\n",
    "    process = subprocess.Popen(upload_source_command,\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE,\n",
    "                         env=my_env)\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    output = stdout.decode('utf-8').split('\\n')\n",
    "    error = stderr.decode('utf-8')\n",
    "    print(output)\n",
    "    \n",
    "# upload_source(market_file_prefix, stats_type, published_data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13305e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_recipe(market_file_prefix, stats_type, working_directory):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(working_directory)\n",
    "                # print(f'tilesets create stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data --recipe {market_file_prefix}_bg_{stats_type.lower()}_recipe.json --name \"{market_file_prefix}_bg_{stats_type.lower()}_data\"')\n",
    "    update_recipe_command = [\n",
    "        'tilesets',\n",
    "        'update-recipe',\n",
    "        f'stobieb.{market_file_prefix}_{stats_type.lower()}',\n",
    "        f'{market_file_prefix}_{stats_type.lower()}_recipe.json',\n",
    "        \n",
    "    ]\n",
    "\n",
    "    print(' '.join(update_recipe_command))\n",
    "    process = subprocess.Popen(update_recipe_command,\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE,\n",
    "                         env=my_env)\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    output = stdout.decode('utf-8').split('\\n')\n",
    "    error = stderr.decode('utf-8')\n",
    "    print(output, error)\n",
    "    \n",
    "def upload_tileset(market_file_prefix, stats_type, working_directory):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(working_directory)\n",
    "                # print(f'tilesets create stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data --recipe {market_file_prefix}_bg_{stats_type.lower()}_recipe.json --name \"{market_file_prefix}_bg_{stats_type.lower()}_data\"')\n",
    "    create_tileset_command = [\n",
    "        'tilesets',\n",
    "        'create',\n",
    "        f'stobieb.{market_file_prefix}_{stats_type.lower()}',\n",
    "        '--recipe', \n",
    "        f'{market_file_prefix}_{stats_type.lower()}_recipe.json',\n",
    "        '--name',\n",
    "        f'{market_file_prefix}_{stats_type.lower()}'\n",
    "        \n",
    "    ]\n",
    "\n",
    "    print(' '.join(create_tileset_command))\n",
    "    process = subprocess.Popen(create_tileset_command,\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE,\n",
    "                         env=my_env)\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    output = stdout.decode('utf-8').split('\\n')\n",
    "    error = stderr.decode('utf-8')\n",
    "    print(output, error)\n",
    "\n",
    "def upload_label_tileset(market_file_prefix, stats_type, working_directory):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(working_directory)\n",
    "                # print(f'tilesets create stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data --recipe {market_file_prefix}_bg_{stats_type.lower()}_recipe.json --name \"{market_file_prefix}_bg_{stats_type.lower()}_data\"')\n",
    "    create_tileset_command = [\n",
    "        'tilesets',\n",
    "        'create',\n",
    "        f'stobieb.{market_file_prefix}_{stats_type.lower()}',\n",
    "        '--recipe', f'{market_file_prefix}_{stats_type.lower()}_recipe.json',\n",
    "        '--name',\n",
    "        f'{market_file_prefix}_{stats_type.lower()}'\n",
    "        \n",
    "    ]\n",
    "\n",
    "    print(' '.join(create_tileset_command))\n",
    "    process = subprocess.Popen(create_tileset_command,\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE,\n",
    "                         env=my_env)\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    output = stdout.decode('utf-8').split('\\n')\n",
    "    error = stderr.decode('utf-8')\n",
    "    print(output, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_tileset(market_file_prefix, stats_type, published_data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def publish_tileset(market_file_prefix, stats_type, working_directory):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(working_directory)\n",
    "                # print(f'tilesets create stobieb.{market_file_prefix}_bg_{stats_type.lower()}_data --recipe {market_file_prefix}_bg_{stats_type.lower()}_recipe.json --name \"{market_file_prefix}_bg_{stats_type.lower()}_data\"')\n",
    "    publish_tileset_command = [\n",
    "        'tilesets',\n",
    "        'publish',\n",
    "        f'stobieb.{market_file_prefix}_{stats_type.lower()}'\n",
    "    ]\n",
    "\n",
    "    print(' '.join(publish_tileset_command))\n",
    "    process = subprocess.Popen(publish_tileset_command,\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE,\n",
    "                         env=my_env)\n",
    "\n",
    "    stdout, stderr = process.communicate()\n",
    "    output = stdout.decode('utf-8').split('\\n')\n",
    "    error = stderr.decode('utf-8')\n",
    "    print(output, error)\n",
    "    result = json.loads(output[0])\n",
    "    print(result)\n",
    "    get_job_status_command = [\n",
    "        'tilesets',\n",
    "        'job',\n",
    "        f'stobieb.{market_file_prefix}_{stats_type.lower()}',\n",
    "        result['jobId']\n",
    "    ]\n",
    "    state = 'processing'\n",
    "    while state == 'processing':\n",
    "        process = subprocess.Popen(get_job_status_command,\n",
    "                     stdout=subprocess.PIPE, \n",
    "                     stderr=subprocess.PIPE,\n",
    "                     env=my_env)\n",
    "\n",
    "        stdout, stderr = process.communicate()\n",
    "        output = stdout.decode('utf-8').split('\\n')\n",
    "        try:\n",
    "            result = json.loads(output[0])\n",
    "        except:\n",
    "            result = {}\n",
    "        if 'stage' in result.keys():\n",
    "            state = result['stage']\n",
    "        if state != 'success':\n",
    "            print(f'{state}: sleeping')\n",
    "            time.sleep(30)\n",
    "    print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689aadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish_tileset('austin', 'count', published_data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf726f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "source_data_base_path = \"/Users/alanmccann/Dropbox/bain/bain-uploads/v6_7th_run Brian Stobie/\"\n",
    "published_data_base_path = '/Users/alanmccann/Dropbox/bain/mtsds-data'\n",
    "csv_file_path = f'{source_data_base_path}/Market_Mapping_Settings.csv'\n",
    "market_rows = []\n",
    "with open(csv_file_path, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        market_rows.append(row)\n",
    "market_count = 0\n",
    "completed_markets = []\n",
    "STATS_TYPES = ['Count','Percent','nshft','pshft','growth']\n",
    "for market_data in market_rows[76:]:\n",
    "    market_file_prefix = market_data['MarketName'].lower()\n",
    "    # if market_data['MarketName'] in ['Texas','DallasFortWorth']:\n",
    "    print('creating tilesets for:', market_file_prefix)\n",
    "    for stats_type in STATS_TYPES:\n",
    "        upload_source(market_file_prefix, stats_type, published_data_base_path)\n",
    "        upload_tileset(market_file_prefix, stats_type, published_data_base_path)\n",
    "        update_recipe(market_file_prefix, stats_type, published_data_base_path)\n",
    "        publish_tileset(market_file_prefix, stats_type, published_data_base_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('albuquerquesantafe_bg_count_data_src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9e202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c596f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb0730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GreensboroWinstonSalem\n",
    "# GreensboroWinston\n",
    "\n",
    "# KingsportBristolJohnsonCity\n",
    "# KingsportBristol\n",
    "\n",
    "# ProvidenceNewBedford\n",
    "# ProvidenceNB\n",
    "\n",
    "# SacramentoStocktonModesto\n",
    "# Sacramento\n",
    "\n",
    "# SanFranciscoOaklandSanJose\n",
    "# SanFrancisco\n",
    "\n",
    "# WilkesBarreScranton\n",
    "# WilkesBarre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1af5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443873e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
